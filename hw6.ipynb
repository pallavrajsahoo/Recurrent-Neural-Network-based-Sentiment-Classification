{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a446f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sentiment analysis with RNN and BERT embedding\n",
    "Ting-Yao Hu, 2021/03\n",
    "\"\"\"\n",
    "\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from utils import load_pretrained_bert, bert_emb_sentence, accuracy\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "\n",
    "idx2label = [\"positive\", \"neutral\", \"negative\"]\n",
    "label2idx = {label: idx for idx, label in enumerate(idx2label)}\n",
    "\n",
    "BERT_EMB_SIZE = 768\n",
    "OUT_HELDOUT_PATH = \"heldout_pred.txt\"\n",
    " \n",
    "\"\"\" Adapted from homework7, spring 2020 \"\"\"\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, rnn_in_dim, rnn_hid_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.rnn_in_dim = rnn_in_dim\n",
    "        self.rnn_hid_dim = rnn_hid_dim\n",
    "\n",
    "        # Layers\n",
    "        self.rnn = nn.RNN(rnn_in_dim, rnn_hid_dim)\n",
    "        self.rnn2logit = nn.Linear(rnn_hid_dim, 3)\n",
    "\n",
    "    def init_rnn_hid(self):\n",
    "        \"\"\"Initial hidden state.\"\"\"\n",
    "        return torch.zeros(1, 1, self.rnn_hid_dim)\n",
    "\n",
    "    def forward(self, feat_seq):\n",
    "        \"\"\"Feeds the words into the neural network and returns the value\n",
    "        of the output layer.\"\"\"\n",
    "        rnn_outs, _ = self.rnn(feat_seq.unsqueeze(1), self.init_rnn_hid())\n",
    "                                      # (seq_len, 1, rnn_hid_dim)\n",
    "        logit = self.rnn2logit(rnn_outs[-1]) # (1 x 3)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de816392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "##   Tasks   ##\n",
    "###############\n",
    "\n",
    "###\n",
    "## - text_fn: input text file name\n",
    "## - out_fn: h5 file name\n",
    "###\n",
    "def save_bert_embedding(text_fn, out_fn):\n",
    "    ### data\n",
    "    texts = [l.strip() for l in open(text_fn, 'r')]\n",
    "\n",
    "    ### save bert embedding to h5 file\n",
    "    h5_obj = h5py.File(out_fn,'w')\n",
    "    dt = h5py.special_dtype(vlen=np.dtype('float32'))\n",
    "    dataset = h5_obj.create_dataset('embedding',(len(texts),), dtype=dt)\n",
    "    dataset2 = h5_obj.create_dataset('token_num',(len(texts),), dtype='int')\n",
    "    model, tokenizer = load_pretrained_bert()\n",
    "    pbar = tqdm.tqdm(texts)\n",
    "    for i, text in enumerate(pbar):\n",
    "        emb, tokens = bert_emb_sentence(text, model, tokenizer)\n",
    "        save_bert_to_h5(h5_obj, i, emb)\n",
    "        continue\n",
    "\n",
    "    h5_obj.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb16865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "## TODO: Task1\n",
    "##  - h5_obj: 'h5 file object'\n",
    "##  - idx: 'int', sentence index (the idx-th sentence)\n",
    "##  - emb: 'torch.FloatTensor', (1 x token_number x bert_embedding_size), bert embedding of the idx-th sentence\n",
    "###\n",
    "def save_bert_to_h5(h5_obj, idx, emb):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "## TODO: Task2\n",
    "##  - h5_obj: 'h5 file object'\n",
    "##  - idx: 'int', sentence index (the idx-th sentence)\n",
    "## output:\n",
    "##  - feat: 'torch.FloatTensor', (token_number x bert_embedding_size)\n",
    "###\n",
    "def load_bert_from_h5(h5_obj, idx):\n",
    "    feat = None\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "## TODO: Task3\n",
    "## - logit: 'torch.FloatTensor'\n",
    "## output:\n",
    "## - pred: 'int'\n",
    "### \n",
    "def pred_from_logit(logit):\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f57f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-train_fn TRAIN_FN] [-train_lab_fn TRAIN_LAB_FN] [-test_fn TEST_FN]\n",
      "                             [-train_h5_fn TRAIN_H5_FN] [-test_h5_fn TEST_H5_FN] [-rnn_hid_dim RNN_HID_DIM]\n",
      "                             [-epochs EPOCHS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\palla\\AppData\\Roaming\\jupyter\\runtime\\kernel-db817c1d-f82a-4b91-915e-6586210dc0a6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palla\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    ###############\n",
    "    ## arguments ##\n",
    "    ###############\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ### args for data\n",
    "    parser.add_argument('-train_fn', default='data/dev_text.txt', type=str)\n",
    "    parser.add_argument('-train_lab_fn', default='data/dev_label.txt', type=str)\n",
    "    parser.add_argument('-test_fn', default='data/heldout_text.txt', type=str)\n",
    "    parser.add_argument('-train_h5_fn', default='data/bert.h5', type=str)\n",
    "    parser.add_argument('-test_h5_fn', default='data/bert_test.h5', type=str)\n",
    "\n",
    "    ### args for classifier training\n",
    "    parser.add_argument(\"-rnn_hid_dim\", default=10, type=int,\n",
    "                        help=\"Dimentionality of RNN hidden state\")\n",
    "    parser.add_argument(\"-epochs\", default=3, type=int,\n",
    "                        help=\"Number of epochs\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ## extracting bert embeddings, saving as .h5 format ##\n",
    "    #####################################################\n",
    "    if not osp.exists(args.train_h5_fn):\n",
    "        save_bert_embedding(args.train_fn, args.train_h5_fn)\n",
    "    if not osp.exists(args.test_h5_fn):\n",
    "        save_bert_embedding(args.test_fn, args.test_h5_fn)\n",
    "\n",
    "\n",
    "    ########################\n",
    "    ## pytorch classifier ##\n",
    "    ########################\n",
    "    clf = Classifier(BERT_EMB_SIZE, args.rnn_hid_dim)\n",
    "    optimizer = optim.Adam(clf.parameters())\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    ################\n",
    "    ##  training  ##\n",
    "    ################\n",
    "    labs = [label2idx[l.strip()] for l in open(args.train_lab_fn,'r')]\n",
    "    h5_obj = h5py.File(args.train_h5_fn, 'r')\n",
    "    clf.train()\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        print('Epoch:', epoch+1)\n",
    "        for idx in range(len(labs)):\n",
    "            lab = labs[idx]\n",
    "            feat = load_bert_from_h5(h5_obj, idx)\n",
    "\n",
    "            logit = clf.forward(feat)\n",
    "            loss = ce_loss(logit, torch.LongTensor([lab]))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    h5_obj.close()\n",
    "\n",
    "\n",
    "    ###############\n",
    "    ### testing ###\n",
    "    ###############\n",
    "    pred_list = []\n",
    "    h5_obj = h5py.File(args.test_h5_fn, 'r')\n",
    "    clf.eval()\n",
    "    test_num = len(h5_obj['embedding'])\n",
    "    for i in range(test_num):\n",
    "        feat = load_bert_from_h5(h5_obj, i)\n",
    "\n",
    "        logit = clf.forward(feat)\n",
    "        pred = pred_from_logit(logit)\n",
    "        pred_list.append(idx2label[pred])\n",
    "    h5_obj.close()\n",
    "\n",
    "    out = open(OUT_HELDOUT_PATH, 'w')\n",
    "    for pred in pred_list: out.write(pred+'\\n')\n",
    "    out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c06e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
